# -*- coding: utf-8 -*-
"""Regression model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/149_YpO0BURP-DXeA1whbLuSmcLGIkj5E
"""

import pandas as pd
import matplotlib.pyplot as plt

df=pd.read_excel('/content/drive/MyDrive/train_data.xlsx')

df.head()

from google.colab import drive
drive.mount('/content/drive')

df['category_property'].unique()

il=['','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']
for i in il:
  replacements = {
    f'house{i}':'House',
    f'House{i}': 'House',
    f'Lower Portion{i}': 'Lower Portion',
    f'Flat{i}': 'Flat',
    f'Farm House{i}': 'Farm_House',
    f'Upper Portion{i}': 'Upper Portion',
    f'Penthouse{i}': 'Pent_House',
    f'Room{i}': 'Room',
    f'PentHouse{i}': 'Pent_House',
  }
  df['category_property'] = df['category_property'].replace(replacements, regex=True)


df['category_property'].unique()

df.rename(columns={'space_occupied': 'space_occupied(Marla)'}, inplace=True)
df.head()

df['space_occupied(Marla)']=df['space_occupied(Marla)'].apply(lambda x: float(x.split()[0].replace(',', '')) * 20 if 'Kanal' in x else float(x.split()[0].replace(',', '')))

df['area'].nunique()

df['salesperson'].nunique()

df['estate_agency'].nunique()

pip install category_encoders

X=df.drop(columns='price')
y=df['price']

from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
X['area'] = encoder.fit_transform(df['area'])

X['listing_date(year)']=X['listing_date'].dt.year
X['listing_date(month)']=X['listing_date'].dt.month
X['listing_date(day)']=X['listing_date'].dt.day

X=pd.get_dummies(X, columns=['category_property'], drop_first=True)

X.head()

X=pd.get_dummies(X,columns=['listing_purpose'],drop_first=True)

X.head()

X.isnull().sum()

X['salesperson'].value_counts()

X['salesperson'].isnull().value_counts()

import pandas as pd
from sklearn.preprocessing import LabelEncoder
import numpy as np
label_encoder = LabelEncoder()
X['salesperson'] = X['salesperson'].fillna('Missing')
encoded_values = label_encoder.fit_transform(X['salesperson'])
X['Sales_Person'] = encoded_values
X['Sales_Person'] = X['Sales_Person'].where(X['salesperson'] != 'Missing', np.nan)
X['Sales_Person'].value_counts()

X['estate_agency'] = X['estate_agency'].fillna('Missing')
encoded_values1 = label_encoder.fit_transform(X['estate_agency'])
X['Estate_Agency'] = encoded_values1
X['Estate_Agency'] = X['Estate_Agency'].where(X['estate_agency'] != 'Missing', np.nan)
X['Estate_Agency'].value_counts()

X=X.drop(columns=['listing_date','estate_agency','salesperson'])

X.columns

X.isnull().sum()

X['no_of_bedrooms'].value_counts()

X['no_of_bedrooms'].value_counts().plot(kind='pie')

X['no_of_bathrooms'].value_counts().plot(kind='pie')

X.columns

X['no_of_bedrooms'].nunique()

# import numpy as np
# import pandas as pd
# from sklearn.impute import SimpleImputer
# from sklearn.model_selection import GridSearchCV
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.impute import KNNImputer
# # from missingpy import MissForest
# from xgboost import XGBRegressor

# features=['no_of_bedrooms','no_of_bathrooms','Sales_Person','Estate_Agency']

# transformer = Pipeline(steps=[
#     ('imputer', SimpleImputer(strategy='median'))  # This will be changed during GridSearch
# ])
# preprocessor = ColumnTransformer(
#     transformers=[
#         ('num', transformer, features),

#     ]
# )
# xgb = XGBRegressor(objective='reg:squarederror', eval_metric='rmse')
# pipeline = Pipeline(steps=[
#     ('preprocessor', preprocessor),
#     ('model', xgb)
# ])

# from tqdm import tqdm

# param_grid = {
#     'preprocessor__num__imputer': [KNNImputer(n_neighbors=5), SimpleImputer(strategy='median')],
#     'model__n_estimators': [100, 200],
#     'model__max_depth': [3, 5],
#     'model__learning_rate': [0.01, 0.1]
# }

# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1,n_jobs=-1)

# # grid_search.fit(X, y)
# n_combinations = len(param_grid['model__n_estimators']) * len(param_grid['model__learning_rate']) * len(param_grid['model__max_depth'])
# n_fits=n_combinations*5
# with tqdm(total=n_fits, desc="Fitting models") as pbar:
#     grid_search.fit(X, y)
#     pbar.update(n_fits)

# print("Best params:", grid_search.best_params_)
# print("Best score:", grid_search.best_score_)

import pandas as pd
from sklearn.impute import KNNImputer

# imputer = KNNImputer(n_neighbors=3)
# X = imputer.fit_transform(X)

features=['no_of_bedrooms','no_of_bathrooms','Sales_Person','Estate_Agency']
imputer = KNNImputer(n_neighbors=3)
X[features] = X[features].apply(pd.to_numeric, errors='coerce')
X_nan = [features]

X_imputed = imputer.fit_transform(X_nan)

X_imputed_df = pd.DataFrame(X_imputed, columns=features)

X[features] = X_imputed_df

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

print("Data types before conversion:")
print(df.dtypes)

columns_with_nan=['no_of_bedrooms','no_of_bathrooms','Sales_Person','Estate_Agency']

for col in columns_with_nan:
    print(f"Unique values in {col}: {X[col].unique()}")

X[columns_with_nan] = X[columns_with_nan].apply(pd.to_numeric, errors='coerce')

print("\nData types after conversion:")
print(df.dtypes)

imputer = KNNImputer(n_neighbors=3)

X_nan = X[columns_with_nan]

X_nan = X_nan.select_dtypes(include=[np.number])

X_imputed = imputer.fit_transform(X_nan)

X_imputed_df = pd.DataFrame(X_imputed, columns=X_nan.columns)

X[columns_with_nan] = X_imputed_df

print("\nDataFrame after imputation:")
print(X)

X.head()

X.isnull().sum()

import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = XGBRegressor(
    n_estimators=5000,
    learning_rate=0.001,
    max_depth=3,
    random_state=42
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse:.2f}')
print(f'RÂ² Score: {r2:.2f}')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train1 = scaler.fit_transform(X_train)
X_test1 = scaler.transform(X_test)

svr = SVR(kernel='linear')
svr.fit(X_train1, y_train)

y_pred = svr.predict(X_test1)<-----yaha per ruk jaoge

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

models = {
    'Linear Regression': LinearRegression(),
    'MLP Regressor': MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),
    'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),
    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),
    'XGBoost Regressor': xgb.XGBRegressor(n_estimators=100, random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{name}:")
    print(f"  Mean Squared Error: {mse}")
    print(f"  R-squared: {r2}")
    print("-" * 30)

# Voting

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf1 = LogisticRegression()
clf2 = KNeighborsClassifier()
clf3 = DecisionTreeClassifier()

voting_clf = VotingClassifier(estimators=[('lr', clf1), ('knn', clf2), ('dt', clf3)], voting='hard')

voting_clf.fit(X_train, y_train)


y_pred = voting_clf.predict(X_test)


# Stacking
from sklearn.ensemble import StackingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

base_models = [
    ('dt', DecisionTreeClassifier()),
    ('svc', SVC(probability=True))
]

meta_model = LogisticRegression()

stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)

stacking_clf.fit(X_train, y_train)
y_pred = stacking_clf.predict(X_test)


# Blending
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

X_train_part1, X_train_part2, y_train_part1, y_train_part2 = train_test_split(X_train, y_train, test_size=0.5, random_state=42)

clf1 = LogisticRegression()
clf2 = DecisionTreeClassifier()

clf1.fit(X_train_part1, y_train_part1)
clf2.fit(X_train_part1, y_train_part1)

pred1 = clf1.predict(X_train_part2)
pred2 = clf2.predict(X_train_part2)

meta_features = pd.DataFrame({'pred1': pred1, 'pred2': pred2})

meta_model = LogisticRegression()
meta_model.fit(meta_features, y_train_part2)

test_pred1 = clf1.predict(X_test)
test_pred2 = clf2.predict(X_test)
test_meta_features = pd.DataFrame({'pred1': test_pred1, 'pred2': test_pred2})
y_pred = meta_model.predict(test_meta_features)


# multi layer ensemble
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

base_models = [
    ('svc', SVC(probability=True)),
    ('dt', DecisionTreeClassifier()),
    ('lr', LogisticRegression())
]

meta_model_1 = RandomForestClassifier()

stacking_layer_1 = StackingClassifier(estimators=base_models, final_estimator=meta_model_1)

stacking_layer_1.fit(X_train, y_train)

first_layer_predictions = stacking_layer_1.predict_proba(X_train)

meta_model_2 = LogisticRegression()

meta_model_2.fit(first_layer_predictions, y_train)

first_layer_test_predictions = stacking_layer_1.predict_proba(X_test)
final_predictions = meta_model_2.predict(first_layer_test_predictions)

print(f"Final Predictions: {final_predictions}")